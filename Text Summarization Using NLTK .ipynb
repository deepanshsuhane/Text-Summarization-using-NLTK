{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0174c9",
   "metadata": {},
   "source": [
    "# AISC2009 - Natural Language Processing 01\n",
    "## In-Class Application Exercise 4\n",
    "## Student Name & ID\n",
    "### Nikhil Nikhare - 500197137\n",
    "### Deepansh Suhane - 500201397\n",
    "### Muhammad Ahmer - 500196858\n",
    "### Jaswinder Singh - 500195345\n",
    "### Syed Jafri - 500196461"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147eb2d",
   "metadata": {},
   "source": [
    "# **Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84f494",
   "metadata": {},
   "source": [
    "**Step 1** \n",
    "\n",
    "Firstly, we retrieve an article from Wikipedia (here we have used Loyalist College Article) in order to summarise them. Then, we have utilised few libraries in order to achieve our goal. The first package that needs to be downloaded is called beautiful soup which is an important tool for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4160cb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/deepanshsuhane/opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/deepanshsuhane/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1560d30",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "Now we have installed lxml library which is used to scrape data from the web and it also helped us to handle XML and HTML files with an ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ede3c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/deepanshsuhane/opt/anaconda3/lib/python3.9/site-packages (4.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc3017",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "\n",
    "Here we have used bs4 which stands beautiful soup 4 which is the latest version of the library. We have also used urllib.request library as it is an extensible library when opening an URL and re library for importing regular expression.\n",
    "\n",
    "We will now import an article from Wikipedia as given below.\n",
    "\n",
    "The idea is to cut down on the amount of time you spend reading (regardless of where the text originates from). \n",
    "Hence, we have created a tool that is flexible enough to be used to numerous sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3043a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Loyalist_College')\n",
    "article = scraped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61107c",
   "metadata": {},
   "source": [
    "**Step 4**\n",
    "\n",
    "We will now remove Square Brackets and Extra Spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56b5a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23854971",
   "metadata": {},
   "source": [
    "**Step 5**\n",
    "\n",
    "Removing Special Characters and Digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cf19e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106980b6",
   "metadata": {},
   "source": [
    "**Step 6**\n",
    "\n",
    "Now we will import nltk ibrary as it offers us a variety of text processing libraries and helped us in Tokenizing and many other activities.\n",
    "\n",
    "We have also installed \"popular\" subset of NLTK data set which helped us to implement our code in a streamlined way. So, if you're unsure which datasets or models you'll require, you must use this subset of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37277f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/deepanshsuhane/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2c3c2",
   "metadata": {},
   "source": [
    "**Step 7**\n",
    "\n",
    "We've already preprocessed the data at this point. We will now execute tokenizer package as it helped us to divide a string into sub-strings. These tokens or sub-strings are excellent for identifying patterns and can be considered as the first step in stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8f977e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e913f0b",
   "metadata": {},
   "source": [
    "**Step 8**\n",
    "\n",
    "Moving forward, we will now remove stop words (Frequently used words like \"the,\" \"a,\" \"an,\" and \"in\" are examples of stop words).\n",
    "\n",
    "We also need to find the frequency of occurence of each words. Hence, we have used the formatted_article_text variable so as to determine how frequently each word appears. Since it lacks punctuation, numerals, or any other special characters, we used this variable to determine the frequency of occurrence.\n",
    "\n",
    "*Understanding our if else loop:-*\n",
    "\n",
    "To determine if there are any stop words, we have created a loop which runs through each phrase and then the related terms. \n",
    "If not, we then determine whether the terms are included in the word frequency dictionary or not. The dictionary adds the word as a key and sets its value to 1 if it is encountered for the first time. \n",
    "Alternatively, if the word was already in the dictionary, its value is simply increased by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64aa6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a5a92",
   "metadata": {},
   "source": [
    "**Step 9**\n",
    "\n",
    "*Calculating the weighted frequency:* Here, we have divided the total number of occurrences by the frequency of the term that appears the most frequently to obtain the weighted frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fbc091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d0ef3",
   "metadata": {},
   "source": [
    "**Step 10**\n",
    "\n",
    "*Calculating Sentence Score:* We have combined the weighted frequencies of the words that appear in each sentence to calculate a sentense score.\n",
    "\n",
    "*Understanding our second if else loop:-* \n",
    "\n",
    "First, a blank sentence_scores dictionary is created. The sentences themselves will serve as the dictionary's keys, and the sentences associated scores will serve as the dictionary's values. The sentences in the sentence_list are then tokenized into words using a loop through each sentence.\n",
    "\n",
    "The next step is to see if the word is listed in the dictionary word frequencies. This check is made because the word frequencies were calculated using the formatted_article_text_object, which does not contain any stop words, numbers, etc., whereas the sentence list list was constructed from the article_text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf3f263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebe1f6",
   "metadata": {},
   "source": [
    "**Step 11**\n",
    "\n",
    "**Final Step to get the Summary**\n",
    "\n",
    "Till here we have the sentence_scores dictionary which constitute corresponding score. Now to summarize our article, we took top N scores based on their highest scores.\n",
    "\n",
    "We will now recall the nlargest function of the heapq library to extract the top seven phrases with the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cffcc630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loyalist College houses some of its students in the Loyalist College Residence located on the college campus. This building contains laundry facilities, computer access, security office, food station and entertainment area. Originally operated out of a local high school, Loyalist College moved to its present 200-acre (81 ha) campus on Wallbridge-Loyalist Road in 1968. Loyalist College (formally Loyalist College of Applied Arts and Technology) is an English-language college in Belleville, Ontario, Canada. The name of the college reflects the area's original settlement by United Empire Loyalists. The entertainment area includes pool tables, ping-pong and television for student use. The college operates a satellite campus in Bancroft, Ontario and is associated with First Nations Technical Institute in the Tyendinaga Mohawk Territory reserve.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce437c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
